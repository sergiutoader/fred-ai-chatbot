# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

app:
  name: "Knowledge Flow Backend"
  base_url: "/knowledge-flow/v1"
  address: "127.0.0.1"
  port: 8111
  log_level: "info"
  reload: false
  reload_dir: "."

processing:
  generate_summary: false
  use_gpu: true
  process_images: false

security:
  m2m:
    enabled: false
    client_id: "knowledge-flow"
    realm_url: "http://app-keycloak:8080/realms/app"
  user:
    enabled: false
    client_id: "app"
    realm_url: "http://app-keycloak:8080/realms/app"
    authorized_origins:
      - "http://localhost:5173"
  rebac:
    type: spicedb
    endpoint: "http://localhost:50051"
    insecure: true

scheduler:
  enabled: true
  backend: "temporal"
  temporal:
    host: "localhost:7233"
    namespace: "default"
    task_queue: "ingestion"
    workflow_prefix: "pipeline"
    connect_timeout_seconds: 5

input_processors:
  - prefix: ".pdf"
    class_path: app.core.processors.input.pdf_markdown_processor.pdf_markdown_processor.PdfMarkdownProcessor
  - prefix: ".docx"
    class_path: app.core.processors.input.docx_markdown_processor.docx_markdown_processor.DocxMarkdownProcessor
  - prefix: ".pptx"
    class_path: app.core.processors.input.pptx_markdown_processor.pptx_markdown_processor.PptxMarkdownProcessor
  - prefix: ".csv"
    class_path: app.core.processors.input.csv_tabular_processor.csv_tabular_processor.CsvTabularProcessor
  - prefix: ".txt"
    class_path: app.core.processors.input.text_markdown_processor.text_markdown_processor.TextMarkdownProcessor
  - prefix: ".md"
    class_path: app.core.processors.input.markdown_markdown_processor.markdown_markdown_processor.MarkdownMarkdownProcessor
  - prefix: ".xlsm"
    class_path: app.core.processors.input.pps_tabular_processor.pps_tabular_processor.PpsTabularProcessor
  - prefix: ".jsonl"
    class_path: app.core.processors.input.jsonl.jsonl_markdown_processor.JsonlMarkdownProcessor

content_storage:
  type: "local"
  root_path: ~/.fred/knowledge-flow/content-storage

document_sources:
  fred:
    type: push
    description: "Documents manually uploaded by users"

  local:
    type: pull
    provider: local_path
    base_path: ~/Documents/Fred
    description: "Personal local documents available for pull-mode ingestion"

embedding:
  provider: openai
  name: text-embedding-3-large
  settings: {}

# Configuration for the LLM used for summary, keyword generation
# and other text generation tasks.
model:
  provider: openai # openai | azure | ollama
  name: gpt-4o-mini # azure: deployment name, ollama: 'qwen2.5:3b-instruct' etc.
  settings:
    # openai: nothing else required (keys in .env)
    # azure: { endpoint: "...", api_version: "2024-08-01-preview" }
    # ollama: { base_url: "http://localhost:11434" }

# model:
#   provider: ollama
#   name: qwen2.5:3b-instruct         # any chat-capable Ollama model you pulled
#   settings:
#     base_url: http://localhost:11434
#     temperature: 0.7                # optional, forwarded to LangChain Ollama wrapper

# model:
#   provider: azure
#   # azure use deployment name instead of a plain model name as openai. The principle
#   # is the same.
#   name: fred-gpt-4o              # Azure deployment name for your chat model
#   settings:
#     azure_endpoint: https://<your-azure-openai>.openai.azure.com
#     api_version: 2024-05-01-preview

# model:
#   provider: azureapim        # openai | azure | ollama
#   # azure use deployment name instead of a plain model name as openai. The principle
#   # is the same.
#   name: fred-gpt-4o       # azure: deployment name, ollama: 'qwen2.5:3b-instruct' etc.
#   settings:
#     azure_apim_base_url: "https://tehopenai.openai.azure.com/"
#     azure_resource_path: "/genai-aoai-inference/v2"
#     azure_api_version: "2024-06-01"
#     azure_tenant_id: "737c6905-f186-4bcf-afb3-43e349ee23a3"
#     azure_client_id: "5e5d67c9-22af-4044-bc8e-5df58d7a5e6e"
#     azure_client_scope: "api://5e5d67c9-22af-4044-bc8e-5df58d7a5e6e/.default"

storage:
  postgres:
    host: localhost
    port: 5432
    database: fred
    username: admin

  opensearch:
    host: https://localhost:9200
    secure: true
    verify_certs: false
    username: admin

  catalog_store:
    type: "duckdb"
    duckdb_path: "~/.fred/knowledge-flow/catalog.duckdb"

  prompt_store:
    type: "duckdb"
    duckdb_path: "~/.fred/knowledge-flow/prompt.duckdb"

  resource_store:
    type: "duckdb"
    duckdb_path: "~/.fred/knowledge-flow/resource.duckdb"

  kpi_store:
    type: "log"
    level: "INFO"

  tag_store:
    type: "duckdb"
    duckdb_path: "~/.fred/knowledge-flow/tag.duckdb"

  metadata_store:
    type: "duckdb"
    duckdb_path: "~/.fred/knowledge-flow/metadata.duckdb"

  vector_store:
    type: chroma

  tabular_stores:
    base_database: # Minimal configuration to enable CSV document ingestion
      type: "sql"
      driver: "duckdb"
      database: "base_database"
      path: "~/.fred/knowledge-flow/db.duckdb"
      mode: "read_and_write"

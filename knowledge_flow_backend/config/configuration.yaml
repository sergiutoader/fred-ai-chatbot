# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

app:
  name: "Knowledge Flow Backend"
  base_url: "/knowledge-flow/v1"
  address: "127.0.0.1"
  port: 8111
  log_level: "info"
  reload: false
  reload_dir: "."


# Enable or disable the security layer
security:
  enabled: false
  keycloak_url: "http://fred-keycloak:8080/realms/fred"
  authorized_origins:
  - "http://localhost:5173"

scheduler:
  enabled: true
  backend: "temporal"
  temporal:
    host: "localhost:7233"
    namespace: "default"
    task_queue: "ingestion"
    workflow_prefix: "pipeline"
    connect_timeout_seconds: 5

# -----------------------------------------------------------------------------
# INPUT PROCESSORS
# -----------------------------------------------------------------------------
# Mandatory: Input processors MUST be explicitly defined.
# These classes parse incoming files (e.g., PDFs, DOCXs, CSVs) into structured documents.

input_processors:
  - prefix: ".pdf"
    class_path: app.core.processors.input.pdf_markdown_processor.openai_pdf_markdown_processor.OpenaiPdfMarkdownProcessor
  - prefix: ".docx"
    class_path: app.core.processors.input.docx_markdown_processor.docx_markdown_processor.DocxMarkdownProcessor
  - prefix: ".pptx"
    class_path: app.core.processors.input.pptx_markdown_processor.pptx_markdown_processor.PptxMarkdownProcessor
  - prefix: ".csv"
    class_path: app.core.processors.input.csv_tabular_processor.csv_tabular_processor.CsvTabularProcessor
  - prefix: ".txt"
    class_path: app.core.processors.input.text_markdown_processor.text_markdown_processor.TextMarkdownProcessor
  - prefix: ".md"
    class_path: app.core.processors.input.markdown_markdown_processor.markdown_markdown_processor.MarkdownMarkdownProcessor
  - prefix: ".xlsm"
    class_path: app.core.processors.input.pps_tabular_processor.pps_tabular_processor.PpsTabularProcessor


# -----------------------------------------------------------------------------
# OUTPUT PROCESSORS (Optional)
# -----------------------------------------------------------------------------
# Optional: You can override the default behavior for output processing.
# If not defined, the system automatically selects based on input type:
#   - Markdown files → VectorizationProcessor
#   - Tabular files (CSV, XLSX) → TabularProcessor
#
# You can specialize behavior by mapping file extensions to custom classes.
#
# Example to OVERRIDE default behavior:
#
# output_processors:
#   - prefix: ".docx"
#     class_path: app.core.processors.output.vectorization_processor.vectorization_processor.VectorizationProcessor
#   - prefix: ".csv"
#     class_path: app.core.processors.output.tabular_processor.tabular_processor.TabularProcessor
#   - prefix: ".xlsx"
#     class_path: app.core.processors.output.tabular_processor.tabular_processor.TabularProcessor
#   - prefix: ".pptx"
#     class_path: app.core.processors.output.vectorization_processor.vectorization_processor.VectorizationProcessor
#
# To SKIP processing for a specific file type, you can specify an empty output processor.
#
# output_processors:
#  - prefix: ".txt"
#    class_path: app.core.processors.output.empty_output_processor.EmptyOutputProcessor

content_storage:
  # The content store type can be either "local" or "minio" or "gcs"
  # If you are using minio, make sure to set the following environment variables:
  # - MINIO_ACCESS_KEY
  # - MINIO_SECRET_KEY
  # If you are using gcs, make sure to set the following environment variables:
  # - GCS_PROJECT_ID
  # - GCS_BUCKET_NAME
  # - GCS_CREDENTIALS_PATH
  # If you are using local storage, make sure to set the following environment variable:
  # - LOCAL_CONTENT_STORAGE_PATH default to '~/.knowledge-flow/content-store'
  type: "local"
  root_path: ~/.knowledge-flow/content-storage
  ################################################
  # Example using MinIO as content storage backend
  ################################################
  # type: "minio"
  # endpoint: localhost:9000
  # access_key: minioadmin # Overrides the MINIO_ACCESS_KEY environment variable
  # secret_key: minioadmin # Overrides the MINIO_SECRET_KEY environment variable
  # secure: False
  # bucket_name: app-bucket

document_sources:
  uploads:
    type: push
    description: "Documents manually uploaded by users"
  local-docs:
    type: pull
    provider: local_path
    base_path: ~/Documents
    description: "Personal local documents available for pull-mode ingestion"
  team-github:
    type: pull
    provider: github
    repo: myorg/docs
    description: "Team documentation from GitHub"

metadata_storage:
  # Metadata Storage Configuration
  #
  # Available backends:
  # - local       → Deprecated, stores metadata in a JSON file on disk
  # - duckdb      → stores metadata in a local duckdb file
  # - opensearch  → stores metadata in a persistent OpenSearch index (recommended for production)
  #
  # ✅ Default is "local" for easy startup. No external services are needed.
  # If you are using local storage, make sure to set the following environment variable:
  # - LOCAL_METADATA_STORAGE_PATH default to '~/.knowledge-flow/metadata-store.json'
  type: "duckdb"
  root_path: ~/.knowledge-flow/db.duckdb

  # --- Example: SearchEngine configuration
  #
  # type: opensearch
  # host: https://localhost:9200              # SearchEngine host
  # secure: true                              # Use HTTPS (set to false for HTTP)
  # verify_certs: false                       # Whether to verify TLS certificates
  # metadata_index: metadata-index            # Index for storing metadata
  # vector_index: vector-index                # Vector index (required if reusing OpenSearch backend)
  # ➤ Required environment variables (not stored in this file):
  #   OPENSEARCH_USER=admin
  #   OPENSEARCH_PASSWORD=xxx

tag_storage:
  type: "local"

vector_storage:
  opensearch:
    host: "http://localhost:9200"
    metadata_index: "metadata-index"
  # Available backends:
  # - in_memory  → ephemeral, dev-only (no persistence)
  # - opensearch → persistent, secure, production-ready
  # - weaviate   → persistent, fast, lightweight alternative. Still beta
  #
  # ⚠️ in_memory is used by default to ensure startup always succeeds, but it does NOT persist vectors between restarts.
  # Use it only for testing or development.
  type: in_memory
  # --- Example: SearchEngine configuration
  #
  # type: opensearch
  # host: https://localhost:9200              # Opensearch HTTP URL
  # secure: true                              # Use HTTPS (set to false for HTTP)
  # verify_certs: false                       # Whether to verify TLS certificates
  # vector_index: vector-index                # Index name for vectors
  # metadata_index: metadata-index            # Index name for metadata
  #
  # ➤ Required environment variables (not stored in this file):
  # OPENSEARCH_USER=admin
  # OPENSEARCH_PASSWORD=xxx
  #
  # --- Example: Weaviate configuration
  #
  # type: weaviate
  # host: http://localhost:8080               # Weaviate HTTP host
  # index_name: CodeDocuments                 # Class name used to store chunks
  #

tabular_storage:
  type: "duckdb"
  duckdb_path: "~/.knowledge-flow/db.duckdb"

catalog_storage:
  type: "duckdb"
  duckdb_path: "~/.knowledge-flow/db.duckdb"

embedding:
  # -----------------------------------------------------------------------------
  # EMBEDDING BACKEND
  # -----------------------------------------------------------------------------
  # Set the embedding backend to use:
  #   - "openai"      → Use OpenAI's public API
  #   - "azureopenai" → Use Azure OpenAI service directly
  #   - "azureapim"   → Use Azure OpenAI via Azure APIM Gateway (OAuth2 + subscription key)
  #   - "ollama"      → Use Ollama's API
  #
  # Required environment variables based on the selected backend:
  #
  # BACKEND: "openai"
  # -------------------------------------
  # - OPENAI_API_KEY
  # - OPENAI_API_BASE (optional if using default)
  # - OPENAI_API_VERSION (optional)
  #
  # BACKEND: "azureopenai"
  # -------------------------------------
  # - AZURE_OPENAI_API_KEY
  # - AZURE_OPENAI_BASE_URL
  # - AZURE_API_VERSION
  # - AZURE_DEPLOYMENT_EMBEDDING
  #
  # BACKEND: "azureapim"
  # -------------------------------------
  # - AZURE_TENANT_ID
  # - AZURE_CLIENT_ID
  # - AZURE_CLIENT_SECRET
  # - AZURE_CLIENT_SCOPE
  # - AZURE_APIM_BASE_URL
  # - AZURE_APIM_KEY
  # - AZURE_API_VERSION
  # - AZURE_RESOURCE_PATH_EMBEDDINGS
  # - AZURE_DEPLOYMENT_EMBEDDING
  #
  # BACKEND: "ollama"
  # -------------------------------------
  # - OLLAMA_API_URL (optional)
  # - OLLAMA_EMBEDDING_MODEL_NAME
  # - OLLAMA_VISION_MODEL_NAME (optional, for vision tasks)
  #
  # All environment variables are expected to be present in the .env file
  # pointed to by the ENV_FILE variable in your Makefile.
  #
  type: "openai"  # can be "openai" or "azureopenai" or "azureapim" or "ollama"

knowledge_context_storage:
  # -----------------------------------------------------------------------------
  # KNOWLEDGE CONTEXT STORAGE BACKEND
  # -----------------------------------------------------------------------------
  # Backend used to store Knowledge Contexts (Workspaces) and user profiles.
  # Both can have associated files.
  # -------------------------------------
  type: local  # as of now only local storage is supported
  local_path: "~/.fred/knowledge-context"

knowledge_context_max_tokens: 8000
# -----------------------------------------------------------------------------
# TOKEN LIMIT PER KNOWLEDGE CONTEXT
# -----------------------------------------------------------------------------
# Maximum total token count allowed for all documents in a single knowledge context.
# This limit ensures consistent performance and avoids processing overhead.
# If the total tokens of uploaded documents exceeds this value,
# the upload or update will fail with a clear error (400).
#
# Use a value aligned with your model capabilities (e.g. 8192 for GPT-4 Turbo).
